{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning and deep probabalistic ensembles\n",
    "\n",
    "[Active learning](http://www.cs.northwestern.edu/~pardo/courses/mmml/papers/active_learning/improving_generalization_with_active_learning_ML94.pdf), loosely described, is an iterative process for getting the most out of your training data. This is especially useful for cases where you have a lot of unlabeled data that you would like to use for [supervised training](https://en.wikipedia.org/wiki/Supervised_learning). Since labeling data can be extremely time consuming and costly, you want to choose which data points to label strategically so that you total training set consists of a rich and diverse set of examples that minimize redundancy and maximize generalization.\n",
    "\n",
    "Last week at PyData Miami, Cl√©ment Farabet of NVIDIA discussed some of their researchers' work where they introduce a method for training a \"[deep probabalistic ensemble](https://arxiv.org/pdf/1811.03575.pdf)\" and apply the ensemble to active learning.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Why \"deep probabalistic ensembles\"? To answer this question, let's take a step back. and consider a common pattern for active learning. Let's suppose we have two data sets at hand, $D_{L}=\\{X_{L}, y_{L}\\}$ and $D_{U}=\\{X_{U}\\}$, where the first data set is labeled and the second is not. Additionally suppose we have a model $f: X\\rightarrow y$ and a function $u: f(x)\\rightarrow \\mathbb{R}$ that measures the uncertainty of a given prediction $f(x)$. We can train $f$ on $D_{L}$ and exploit the uncertainty of the predictions $\\{f(x); x\\in X_{U}\\}$ to determine which data points in $X_{U}$ should be labeled next. For example, a sketch of this algorithm looks like\n",
    "\n",
    "1. Train $f$ on $D_{L}$.\n",
    "2. Obtain the predictions $\\{f(x); x\\in X_{U}\\}$.\n",
    "3. Use $u$ to calculate the uncertainty of each prediction.\n",
    "4. Select the top $n$ data points in $X_{U}$ where the model's predictions are most uncertain.\n",
    "\n",
    "After labeling these points we can update $D_{L}$ and repeat the process as desired.\n",
    "\n",
    "Of course, since we've mentioned the word \"uncertainty\" several times by now it should be clear why we are interested in deep probabalistic networks. Theoretically we can define a deep neural network \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(w \\ \\vert \\ x)=\\frac{P(w)P(w\\ \\vert \\ x)}{P(x)}\n",
    "\\label{eq:posterior}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $w$ is the set of all weights in the network for which we define the prior $P(w)$. From such a model we could obtain not only predictions of our target variable $y$ but also a measure of uncertainty for those predictions.\n",
    "\n",
    "However, it is well known that training a deep network like this is a difficult, if not impossible, task. Enter \"deep probabalistic ensembles\" which are an approximation of $P(w \\ \\vert \\ x)$.\n",
    "\n",
    "## Approximating deep bayesian neural networks\n",
    "\n",
    "The NVIDIA paper is pretty short and self explanitory so I'll only cover the details briefly in this post.\n",
    "\n",
    "The key is to train an ensemble of neural networks, leveraging existing architectures such as [ResNet](https://arxiv.org/pdf/1512.03385.pdf), together with a single loss function that causes the ensemble to approximate samples from the posterior in \\eqref{eq:posterior}.\n",
    "\n",
    "More precicely, the authors begin by defining the usual objective used in variational inference\n",
    "\n",
    "$$\n",
    "q^{*}=\\underset{q}{\\text{arg min}} \\mathbb{E}\\left [ \\text{log }{\\frac{q(w)}{p(w\\ \\vert \\ x)}} \\right]\n",
    "$$\n",
    "\n",
    "With some algebra they rewrite the objective as\n",
    "\n",
    "$$\n",
    "KL(q(w)\\vert\\vert p(w)) - \\mathbb{E}\\left[\\text{log }p(x \\ \\vert \\ w )\\right] + \\text{log }(p(x)\n",
    "$$\n",
    "\n",
    "To make the objective computationally tractable the last term, which is independent of $w$, is removed, resulting in a new objective, the [Evidence Lower Bound](https://en.wikipedia.org/wiki/Evidence_lower_bound),\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "KL(q(w)\\vert\\vert p(w)) - \\mathbb{E}\\left[\\text{log }p(x \\ \\vert \\ w )\\right]\n",
    "\\label{eq:objective}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep probabalistic ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from resnet import ResNet\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def kl_regularization(layers):\n",
    "    layers = K.stack(layers, axis=0)\n",
    "    layer_dims = K.cast_to_floatx(K.int_shape(layers[0]))\n",
    "    n_w = layer_dims[0]\n",
    "    n_h = layer_dims[1]\n",
    "    n_o = layer_dims[3]\n",
    "    mu_i = K.mean(layers, axis=0)\n",
    "    var_q_i = K.var(layers, axis=0)\n",
    "    var_p_i = 2 / (n_w * n_h * n_o)\n",
    "    kl_i = K.log(var_q_i) + (var_p_i / var_q_i) + (mu_i**2 / var_q_i)\n",
    "    return K.sum(kl_i)\n",
    "\n",
    "\n",
    "def ensemble_crossentropy(y_true, y_pred):\n",
    "    ensemble_entropy = K.categorical_crossentropy(y_true, y_pred, axis=-1)\n",
    "    return K.sum(ensemble_entropy, axis=-1)\n",
    "\n",
    "\n",
    "class Stack(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        return K.stack(X, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # assumes all input shapes are the same\n",
    "        return (input_shape[0][0], len(input_shape), input_shape[0][1])\n",
    "\n",
    "\n",
    "class DeepProbabalisticEnsemble(keras.models.Model):\n",
    "    def __init__(self, input_shape, n_classes, n_members, beta=10**-5):\n",
    "        # build ensemble\n",
    "        # instantiate the first member of the ensemble so we can reuse its input layer\n",
    "        # with the other layers\n",
    "        self.members = [ResNet(input_shape, classes=n_classes, block='basic', repetitions=[2, 2, 2, 2])]\n",
    "        self.members += [ResNet((32, 32, 3), classes=10, block='basic', input_layer=self.members[0].inputs[0],\n",
    "                                repetitions=[2, 2, 2, 2])\n",
    "                         for _ in range(n_members-1)]\n",
    "        outputs = Stack()([m.output for m in self.members])\n",
    "        self.beta = beta\n",
    "        super().__init__(inputs=self.members[0].inputs, outputs=outputs)\n",
    "    \n",
    "    @property\n",
    "    def losses(self):\n",
    "        losses = super().losses\n",
    "\n",
    "        # compute KL regularization\n",
    "        conv_layers = [\n",
    "            # kernel is index 0, bias is index 1\n",
    "            [L.trainable_weights[0] for L in m.layers if isinstance(L, keras.layers.Conv2D)]\n",
    "            for m in self.members]\n",
    "        # currently, each sublist is a list of model layers.\n",
    "        # realign these sublists to correspond to layers\n",
    "        conv_layers = [[L for L in layers] for layers in zip(*conv_layers)]\n",
    "        kl_regularizations = [self.beta * kl_regularization(layers) for layers in conv_layers]\n",
    "        losses.extend(kl_regularizations)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpe = DeepProbabalisticEnsemble((32, 32, 3), 10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(dpe).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate active learning\n",
    "\n",
    "Below is a simulation of use the ensemble for active learning on the CIFAR10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets.cifar10 import load_data\n",
    "import numpy as np\n",
    "from unittest import mock\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_test = (X_test - X_test.mean(axis=0)) / X_train.std(axis=0)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_train = y_train[:, np.newaxis, :]\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "y_test = y_test[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 1, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 32, 32, 3), (10000, 1, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building ensemble\n",
      "training ensemble\n",
      "trained for 400 epochs\n",
      "validation loss: -637.9523100585938 validation accuracy: 0.318575\n",
      "calculating uncertainty of predictions\n",
      "adding 4000 examples to training data\n",
      "8000 labeled examples\n",
      "42000 unlabeled examples\n",
      "releasing ensemble from GPU memory\n",
      "building ensemble\n",
      "training ensemble\n",
      "trained for 400 epochs\n",
      "validation loss: -641.5997071289063 validation accuracy: 0.3436375\n",
      "calculating uncertainty of predictions\n",
      "adding 4000 examples to training data\n",
      "12000 labeled examples\n",
      "38000 unlabeled examples\n",
      "releasing ensemble from GPU memory\n",
      "building ensemble\n",
      "training ensemble\n",
      "trained for 400 epochs\n",
      "validation loss: -644.327955859375 validation accuracy: 0.349475\n",
      "calculating uncertainty of predictions\n",
      "adding 4000 examples to training data\n",
      "16000 labeled examples\n",
      "34000 unlabeled examples\n",
      "releasing ensemble from GPU memory\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "# the parameters below replicate the \"linear-4\" experiment from the paper\n",
    "n_ensemble_members = 8\n",
    "budget = 16000\n",
    "n_iterations = 4\n",
    "max_epochs = 400\n",
    "patience = 25\n",
    "# keras param, if non-zero this can blow up the output in the cell below\n",
    "verbosity = 0\n",
    "\n",
    "# simulation\n",
    "dpe_builder = partial(DeepProbabalisticEnsemble, (32, 32, 3), 10, n_ensemble_members)\n",
    "b = budget // n_iterations\n",
    "n_acquisitions = b\n",
    "idx = np.random.choice(len(X_train), size=len(X_train), replace=False)\n",
    "X_labeled, y_labeled = X_train[idx[:b]], y_train[idx[:b]]\n",
    "X_unlabeled, y_unlabeled = X_train[idx[b:]], y_train[idx[b:]]\n",
    "while n_acquisitions < budget:\n",
    "    print('building ensemble')\n",
    "    dpe = dpe_builder()\n",
    "    dpe.compile(loss=ensemble_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print('training ensemble')\n",
    "    history = dpe.fit(\n",
    "        X_labeled, y_labeled,\n",
    "        batch_size=32,\n",
    "        epochs=max_epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[keras.callbacks.ReduceLROnPlateau(patience=25)],\n",
    "        verbose=verbosity)\n",
    "    print('trained for %d epochs' % len(history.history['val_loss']))\n",
    "    print('validation loss:', history.history['val_loss'][-1],\n",
    "          'validation accuracy:', history.history['val_acc'][-1])\n",
    "\n",
    "    # aggregate along the individual model predictions\n",
    "    print('calculating uncertainty of predictions')\n",
    "    p = dpe.predict(X_unlabeled).sum(axis=1)\n",
    "    h_cat = (-p * np.log(p)).sum(axis=1)  # approximation of paper\n",
    "    idx_acquisitions = np.argsort(h_cat)[-b:]\n",
    "    idx_rest = np.argsort(h_ens)[:-b]\n",
    "    \n",
    "    print('adding %d examples to training data' % len(idx_acquisitions))\n",
    "    X_labeled = np.concatenate([X_labeled, X_unlabeled[idx_acquisitions]])\n",
    "    y_labeled = np.concatenate([y_labeled, y_unlabeled[idx_acquisitions]])\n",
    "    X_unlabeled = X_unlabeled[idx_rest]\n",
    "    y_unlabeled = y_unlabeled[idx_rest]\n",
    "    n_acquisitions += b\n",
    "    print('%d labeled examples' % len(X_labeled))\n",
    "    print('%d unlabeled examples' % len(X_unlabeled))\n",
    "\n",
    "    print('releasing ensemble from GPU memory')\n",
    "    K.clear_session()\n",
    "    del dpe\n",
    "    del history\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
